{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN6bHpHzresPYopgUXvDFmY"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "##Language Models, the Chat Format and Tokens"
      ],
      "metadata": {
        "id": "XeSBQtkpDALc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###How does a Large Language Model work?"
      ],
      "metadata": {
        "id": "UFObelUStHkJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In a text generation process, we give a prompt like *'I love eating'* and ask an LLM to fill in what the things are likely to complete this prompt."
      ],
      "metadata": {
        "id": "JT0UqnCvt93Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The main tool used to train an LLM is Supervised Learning. In supervised learning a computer learns the input-output or X and Y mapping using some labelled training data. So the process of supervised learning is typically to get labelled data and then train the model on that data and after training, the model is deployed to be used."
      ],
      "metadata": {
        "id": "VQQs43QquYee"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "An LLM can be built by using supervised learning to repeatedly predict the next word."
      ],
      "metadata": {
        "id": "AIet5-ggxOJq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For example, we have a sentence *'My favorite food is bagel with cream cheese and lox'* in our training data.\n",
        "\n",
        "This sentence is turned into a sequence of training examples, where given a sentence fragment, you want to predict the next word."
      ],
      "metadata": {
        "id": "L4jzDwdWxj-H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For example,\n",
        "\n",
        "My favorite food is **bagel**\n",
        "\n",
        "My favorite food is bagel **with**\n",
        "\n",
        "and so on."
      ],
      "metadata": {
        "id": "-SIqpUpQyCBz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Given a large training set of hundreds or billions of words, we can create a massive training set where we start off with a part of sentence or part of a piece of text and repeatedly ask the language model to learn to predict what the next word is."
      ],
      "metadata": {
        "id": "QyG2nZ66yccf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Two types of Large Language Models(LLMs)"
      ],
      "metadata": {
        "id": "SsAagX6vzCRA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Base LLM**\n",
        "\n",
        "The base LLM repeatedly predicts the next word based on text training data. So, if we give it a prompt *'Once upon a time there was a a unicorn'*, then it may, by repeatedly predicting one word at a time, come up with a completion that tells a story of a unicorn *'that lived in a magical forest.'*\n",
        "\n",
        "A downside of this is that, if we were to prompt it with *'What is the capital of France?'*, quite possible that on internet there might be a list of quiz question about France. So it may complete it with '*What is France's largest city?'* or *'What is France's population?'* and so on.\n",
        "\n",
        "But we want to know the capital of France probably, rather than a bunch of questions related to France.\n",
        "\n",
        "\n",
        "**Instruction Tuned LLM**\n",
        "\n",
        "An instruction tuned LLM, tries to follow the instructions and will answer the above question, i.e. *'What is the capital of France?'* with *'The capital of France is Paris.'*"
      ],
      "metadata": {
        "id": "UMaR_i-tzKcF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###How to go from Base LLM to Instruction Tuned LLM"
      ],
      "metadata": {
        "id": "dCYLJLwQ2QG1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "1.   Train the Base LLM on alot of data, possibly billions of words. This process may take up months on a large supercomputing system.\n",
        "2.   After training the base LLM, we further train the model by fine-tuning it on a smaller set of examples, where the output follows an input instruction. To improve the quality of LLM's output, a common process is to obtain human ratings of the quality of many different LLM outputs, on criteria such as whether it is helpful, honest, and harmless.\n",
        "3. We can then further fine-tune the LLM to increase the probability of its generating the more highly rated outputs using Reinforcement Learning from Human Feedback (RLHF).\n",
        "4. This process of fine tuning can be done in days on a much more modest sized datasets and computaional resources.\n",
        "\n"
      ],
      "metadata": {
        "id": "WgYb6cWp2cCa"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P7fNSfC9soVo"
      },
      "outputs": [],
      "source": [
        "!pip install openai"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "from google.colab import userdata\n",
        "openai.api_key = userdata.get('OPENAI_API_KEY')"
      ],
      "metadata": {
        "id": "sMjgBJWgDP0V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm_model = \"gpt-3.5-turbo-1106\"\n",
        "client = openai.OpenAI(api_key = userdata.get('OPENAI_API_KEY'))"
      ],
      "metadata": {
        "id": "rW8N90M_DaFR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_completion(prompt, model=llm_model):\n",
        "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
        "    response = client.chat.completions.create(\n",
        "        model=model,\n",
        "        messages=messages,\n",
        "        temperature=0,\n",
        "    )\n",
        "    return response.choices[0].message.content"
      ],
      "metadata": {
        "id": "Zj_nybsbDoC1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Prompting model to get a completion"
      ],
      "metadata": {
        "id": "lR_hJHyDFxKg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "response = get_completion(\"What is the capital of France?\")\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HiYHSvDCDrEJ",
        "outputId": "655eba41-64b1-4e0e-fc69-a8c6898eb8e4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The capital of France is Paris.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Tokens"
      ],
      "metadata": {
        "id": "j9hJD_NLGLBg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "If we were to ask the model to take the word '*lollipop*' and reverse it, it outputs a somewhat garbled word."
      ],
      "metadata": {
        "id": "bo11BtN7GYqZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "response = get_completion(\"Take the letters in lollipop \\\n",
        "and reverse them\")\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c9QybzI6Evnr",
        "outputId": "23ccc64f-927c-4092-b97a-51cb041a306b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "pilpolol\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The ChatGPT is unable to do the above simple task because LLM doesn't repeatedly predicts the next word, but it instead repeatedly predicts the next token."
      ],
      "metadata": {
        "id": "sLRFb_k0G-8j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "What an LLM actually does is, it takes a sequence of characters like *'Learning new things is fun!'* and group the characters together to form tokens that consists of commonly occuring sequence of characters. So the sentence *'Learning new things is fun!'* contains all fairly common words, so each token corresponds to one word, therefore it makes 6 tokens."
      ],
      "metadata": {
        "id": "p_DBTUz6HbWw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "But if we were to give it input with some less frequently used words like, *'Prompting is a powerful developer tool.'*, here the word *prompting* is still not that common in english language and so '*prompting*' is actually broken down to 3 tokens - 'prom', 'pt', 'ing', because these 3 are commonly occuring sequences of letters."
      ],
      "metadata": {
        "id": "GiVGsJagJA_u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now if we were to give it the word '*lollipop*', the tokenizer breaks it into 3 tokens - 'l', 'oll', 'ipop'. And because ChatGPT isn't seeing the individual letters, instead it is seeing these 3 tokens, it becomes more difficult for it to correctly print out the letters in reverse order."
      ],
      "metadata": {
        "id": "f_d0-zj8JwWE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "response = get_completion(\"\"\"Take the letters in \\\n",
        "l-o-l-l-i-p-o-p and reverse them\"\"\")\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OG-6W7IhGXIu",
        "outputId": "39b25075-7d15-418c-d66d-ba127f2dc88a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "p-o-p-i-l-l-o-l\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The reason the above prompt (after adding dashes after each letter) works is that the tokenizer now tokenizes each individual character into a different token."
      ],
      "metadata": {
        "id": "ew1zrx1OKxr4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For English lanaguage, 1 token is roughly around 4 characters or 3/4th of the word."
      ],
      "metadata": {
        "id": "Kid2WOKJLZf-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Different LLMs have have different limits on the number of input+output tokens it can accept. The input is often called the context and the output is often called the completion."
      ],
      "metadata": {
        "id": "Mi3-SVWLMKn8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Chat Format"
      ],
      "metadata": {
        "id": "nqrRgjC5NMt1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can separate the system, user and assistant messages using the Chat Format.\n",
        "\n",
        "\n",
        "\n",
        "1.   The system message specifies the overall tone of what we want the LLM to do\n",
        "2.   The user message is a specific instruction that we want to carry out given the higher level behaviour that is specified in the system message.\n",
        "3. We can set the assistant message to let the model know what it had previously said if we want to continue the conversation.\n",
        "\n"
      ],
      "metadata": {
        "id": "pqSGOq6OOIMD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_completion_from_messages(messages,\n",
        "                                 model=\"gpt-3.5-turbo\",\n",
        "                                 temperature=0,\n",
        "                                 max_tokens=500):\n",
        "    response = client.chat.completions.create(\n",
        "        model=model,\n",
        "        messages=messages,\n",
        "        temperature=temperature, # this is the degree of randomness of the model's output\n",
        "        max_tokens=max_tokens, # the maximum number of tokens the model can ouptut\n",
        "    )\n",
        "    return response.choices[0].message.content"
      ],
      "metadata": {
        "id": "dfUiE8FsKu6U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "messages =  [\n",
        "{'role':'system',\n",
        " 'content':\"\"\"You are an assistant who\\\n",
        " responds in the style of Dr Seuss.\"\"\"},\n",
        "{'role':'user',\n",
        " 'content':\"\"\"write me a very short poem\\\n",
        " about a happy carrot\"\"\"},\n",
        "]\n",
        "response = get_completion_from_messages(messages, temperature=1)\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sw14awsQN8EB",
        "outputId": "47ebfb07-0db8-4c52-8014-717e39f3e3b8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "In a garden so bright and cheery,\n",
            "Lived a carrot oh so merry.\n",
            "With a smile wide and so kind,\n",
            "In the sun and breeze, it would unwind.\n",
            "\n",
            "Dancing in the soil so deep,\n",
            "The carrot would laugh and leap.\n",
            "Growing tall and full of glee,\n",
            "Happy as can be, you see!\n",
            "\n",
            "So let's all cheer for this happy carrot,\n",
            "In the garden, shining like a star it!\n",
            "A tale of joy and harvest so fine,\n",
            "For this little carrot, the sun will always shine!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_completion_and_token_count(messages,\n",
        "                                   model=\"gpt-3.5-turbo\",\n",
        "                                   temperature=0,\n",
        "                                   max_tokens=500):\n",
        "\n",
        "    response = client.chat.completions.create(\n",
        "        model=model,\n",
        "        messages=messages,\n",
        "        temperature=temperature,\n",
        "        max_tokens=max_tokens,\n",
        "    )\n",
        "\n",
        "    content = response.choices[0].message.content\n",
        "\n",
        "    token_dict = {\n",
        "'prompt_tokens':response.usage.prompt_tokens,\n",
        "'completion_tokens':response.usage.completion_tokens,\n",
        "'total_tokens':response.usage.total_tokens,\n",
        "    }\n",
        "\n",
        "    return content, token_dict"
      ],
      "metadata": {
        "id": "kQJg6VS8RJwV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "messages = [\n",
        "{'role':'system',\n",
        " 'content':\"\"\"You are an assistant who responds\\\n",
        " in the style of Dr Seuss.\"\"\"},\n",
        "{'role':'user',\n",
        " 'content':\"\"\"write me a very short poem \\\n",
        " about a happy carrot\"\"\"},\n",
        "]\n",
        "response, token_dict = get_completion_and_token_count(messages)"
      ],
      "metadata": {
        "id": "HyABWZ5hTSnV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K-kl9scGTW2I",
        "outputId": "7b7849c2-031d-4c29-c70a-09f3694796a7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Oh, the happy carrot, so bright and so bold,\n",
            "In the garden, it stands tall and never grows old.\n",
            "With a cheerful grin and a vibrant hue,\n",
            "It brings joy to all who see it, it's true!\n",
            "So here's to the carrot, so merry and gay,\n",
            "Spreading happiness in its own special way!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(token_dict)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OuTae6EPVUmx",
        "outputId": "2cea8e3a-325c-437b-a3ed-c1f16a3fe88f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'prompt_tokens': 37, 'completion_tokens': 67, 'total_tokens': 104}\n"
          ]
        }
      ]
    }
  ]
}